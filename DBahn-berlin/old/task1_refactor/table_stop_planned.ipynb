{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import difflib\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import xml.etree.ElementTree as ET\n",
    "from config import cfg\n",
    "\n",
    "try:\n",
    "    # schneller für große Batches\n",
    "    from psycopg2.extras import execute_values\n",
    "except Exception:\n",
    "    execute_values = None\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Einstellungen\n",
    "# ----------------------------\n",
    "TIMEZONE = \"Europe/Berlin\"\n",
    "\n",
    "MATCH_THRESHOLD = 0.85\n",
    "AMBIGUITY_DELTA = 0.02\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "\n",
    "def ensure_stops_table(conn):\n",
    "    create_sql = (cfg.paths.sqls / \"CREATE_STOPS.sql\").read_text(encoding=\"utf-8\")\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(create_sql)\n",
    "    conn.commit()\n",
    "\n",
    "def extract_s_id_and_pts(root: ET.Element):\n",
    "    \"\"\"\n",
    "    Returns: (station_name, rows)\n",
    "    rows: [{ \"id\": <s/@id>, \"ar_pt\": <ar/@pt or None>, \"dp_pt\": <dp/@pt or None> }]\n",
    "    \"\"\"\n",
    "    station_name = root.get(\"station\", None)\n",
    "    if not station_name:\n",
    "        return\n",
    "    rows = []\n",
    "    for s in root.findall(\"s\"):\n",
    "        s_id = s.get(\"id\")\n",
    "\n",
    "        ar = s.find(\"ar\")\n",
    "        dp = s.find(\"dp\")\n",
    "\n",
    "        ar_pt = ar.get(\"pt\") if ar is not None else None\n",
    "        dp_pt = dp.get(\"pt\") if dp is not None else None\n",
    "\n",
    "        rows.append({\"id\": s_id, \"ar_pt\": ar_pt, \"dp_pt\": dp_pt})\n",
    "    return station_name, rows\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Parsing von pt: YYMMDDHHMM -> aware datetime\n",
    "# ----------------------------\n",
    "_pt_re = re.compile(r\"^\\d{10}$\")  # YYMMDDHHMM\n",
    "\n",
    "def parse_pt(pt: str | None, tz: ZoneInfo) -> datetime | None:\n",
    "    if pt is None:\n",
    "        return None\n",
    "    pt = pt.strip()\n",
    "    if not _pt_re.match(pt):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        yy = int(pt[0:2])\n",
    "        year = 2000 + yy\n",
    "        month = int(pt[2:4])\n",
    "        day = int(pt[4:6])\n",
    "        hour = int(pt[6:8])\n",
    "        minute = int(pt[8:10])\n",
    "        return datetime(year, month, day, hour, minute, tzinfo=tz)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Normalisierung & Matching\n",
    "# ----------------------------\n",
    "_umlaut_map = str.maketrans({\"ä\": \"ae\", \"ö\": \"oe\", \"ü\": \"ue\", \"ß\": \"ss\"})\n",
    "_punct_re = re.compile(r\"[^a-z0-9\\s]\")\n",
    "_ws_re = re.compile(r\"\\s+\")\n",
    "\n",
    "_stopwords = {\"berlin\", \"s\", \"u\", \"s+u\", \"u+s\"}\n",
    "\n",
    "def normalize_name(name: str) -> str:\n",
    "    name = name.lower().translate(_umlaut_map)\n",
    "\n",
    "    name = name.replace(\"straße\", \"strasse\")\n",
    "    name = name.replace(\"betriebsbf\", \"betriebsbahnhof\")\n",
    "    name = name.replace(\"hauptbahnhof\", \"hbf\")\n",
    "\n",
    "    name = re.sub(r\"\\bbahnhof\\b\", \"bf\", name)\n",
    "    name = re.sub(r\"\\bbhf\\b\", \"bf\", name)\n",
    "    name = re.sub(r\"\\bbf\\.?\\b\", \"bf\", name)\n",
    "\n",
    "    name = name.replace(\"&\", \" und \")\n",
    "    name = _punct_re.sub(\" \", name)\n",
    "\n",
    "    name = re.sub(r\"(?<=\\w)str\\b\", \"strasse\", name)\n",
    "    name = re.sub(r\"\\bstr\\b\", \"strasse\", name)\n",
    "    name = re.sub(r\"(?<!\\s)strasse\\b\", \" strasse\", name)\n",
    "\n",
    "    name = _ws_re.sub(\" \", name).strip()\n",
    "    tokens = [t for t in name.split() if t and t not in _stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def token_set(s: str) -> set[str]:\n",
    "    return {t for t in s.split() if len(t) >= 2}\n",
    "\n",
    "def similarity(a: str, b: str) -> float:\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    if a == b:\n",
    "        return 1.0\n",
    "\n",
    "    if a in b or b in a:\n",
    "        short, long = (a, b) if len(a) <= len(b) else (b, a)\n",
    "        substring_score = 0.90 + 0.10 * (len(short) / max(1, len(long)))\n",
    "    else:\n",
    "        substring_score = 0.0\n",
    "\n",
    "    seq_score = difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "    ta, tb = token_set(a), token_set(b)\n",
    "    token_score = (2 * len(ta & tb) / (len(ta) + len(tb))) if ta and tb else 0.0\n",
    "\n",
    "    return max(substring_score, seq_score, token_score)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class StationRec:\n",
    "    eva: int\n",
    "    raw_name: str\n",
    "    norm_name: str\n",
    "    tokens: tuple[str, ...]\n",
    "\n",
    "\n",
    "def load_stations(conn) -> list[StationRec]:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT eva, name FROM stationen;\")\n",
    "        rows = cur.fetchall()\n",
    "\n",
    "    stations: list[StationRec] = []\n",
    "    for eva, name in rows:\n",
    "        if not name:\n",
    "            continue\n",
    "        norm = normalize_name(name)\n",
    "        toks = tuple(sorted(token_set(norm)))\n",
    "        stations.append(StationRec(int(eva), name, norm, toks))\n",
    "    return stations\n",
    "\n",
    "\n",
    "def build_token_index(stations: list[StationRec]) -> dict[str, list[int]]:\n",
    "    \"\"\"token -> list of indices into `stations`\"\"\"\n",
    "    idx: dict[str, list[int]] = {}\n",
    "    for i, st in enumerate(stations):\n",
    "        for t in st.tokens:\n",
    "            idx.setdefault(t, []).append(i)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def best_station_match(\n",
    "    query_name: str,\n",
    "    stations: list[StationRec],\n",
    "    token_index: dict[str, list[int]],\n",
    "    threshold: float,\n",
    "    ambiguity_delta: float,\n",
    ") -> tuple[int | None, float, str | None, bool]:\n",
    "    \"\"\"\n",
    "    Returns (eva_or_None, best_score, matched_raw_name, is_ambiguous)\n",
    "    \"\"\"\n",
    "    q_norm = normalize_name(query_name)\n",
    "    if not q_norm:\n",
    "        return None, 0.0, None, False\n",
    "\n",
    "    q_tokens = token_set(q_norm)\n",
    "\n",
    "    cand_indices: set[int] = set()\n",
    "    for t in q_tokens:\n",
    "        cand_indices.update(token_index.get(t, []))\n",
    "\n",
    "    if not cand_indices:\n",
    "        cand_indices = set(range(len(stations)))\n",
    "\n",
    "    best_eva: int | None = None\n",
    "    best_score: float = 0.0\n",
    "    best_name: str | None = None\n",
    "    best_norm: str | None = None\n",
    "    second_best_score: float = 0.0\n",
    "\n",
    "    for i in cand_indices:\n",
    "        st = stations[i]\n",
    "        sc = similarity(q_norm, st.norm_name)\n",
    "\n",
    "        if sc > best_score:\n",
    "            second_best_score = best_score\n",
    "            best_eva, best_score, best_name, best_norm = st.eva, sc, st.raw_name, st.norm_name\n",
    "        elif sc == best_score and best_norm is not None:\n",
    "            if len(st.norm_name) < len(best_norm):\n",
    "                best_eva, best_score, best_name, best_norm = st.eva, sc, st.raw_name, st.norm_name\n",
    "        elif sc > second_best_score:\n",
    "            second_best_score = sc\n",
    "\n",
    "    if best_eva is None or best_score < threshold:\n",
    "        return None, best_score, best_name, False\n",
    "\n",
    "    is_ambiguous = (best_score - second_best_score) < ambiguity_delta\n",
    "    return best_eva, best_score, best_name, is_ambiguous\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# DB Insert/Upsert Stops\n",
    "# ----------------------------\n",
    "UPSERT_SQL = \"\"\"\n",
    "INSERT INTO stops (stop_id, eva, ar_ts, dp_ts)\n",
    "VALUES %s\n",
    "ON CONFLICT (stop_id) DO UPDATE\n",
    "SET eva   = EXCLUDED.eva,\n",
    "    ar_ts = EXCLUDED.ar_ts,\n",
    "    dp_ts = EXCLUDED.dp_ts;\n",
    "\"\"\"\n",
    "\n",
    "def flush_batch(conn, batch: list[tuple], use_execute_values: bool = True) -> int:\n",
    "    if not batch:\n",
    "        return 0\n",
    "    with conn.cursor() as cur:\n",
    "        if use_execute_values and execute_values is not None:\n",
    "            execute_values(cur, UPSERT_SQL, batch, page_size=2000)\n",
    "        else:\n",
    "            upsert_one = \"\"\"\n",
    "            INSERT INTO stops (stop_id, eva, ar_ts, dp_ts)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "            ON CONFLICT (stop_id) DO UPDATE\n",
    "            SET eva = EXCLUDED.eva, ar_ts = EXCLUDED.ar_ts, dp_ts = EXCLUDED.dp_ts;\n",
    "            \"\"\"\n",
    "            cur.executemany(upsert_one, batch)\n",
    "    conn.commit()\n",
    "    return len(batch)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main Import\n",
    "# ----------------------------\n",
    "def import_stops_from_archives(conn, archives_dir: str, pattern: str = \"*.tar.gz\"):\n",
    "    ensure_stops_table(conn)\n",
    "\n",
    "    tz = ZoneInfo(TIMEZONE)\n",
    "\n",
    "    stations = load_stations(conn)\n",
    "    token_index = build_token_index(stations)\n",
    "\n",
    "    unmatched_station_names: set[str] = set()\n",
    "    ambiguous_station_names: set[str] = set()\n",
    "\n",
    "    match_cache: dict[str, tuple[int | None, bool]] = {}  # name -> (eva_or_None, ambiguous)\n",
    "\n",
    "    # dedupe innerhalb eines Batches auf stop_id\n",
    "    batch: dict[str, tuple] = {}  # stop_id -> row\n",
    "\n",
    "    total_upserted = 0\n",
    "    total_seen_stops = 0\n",
    "\n",
    "    for archive_path, _, root in iter_xml_roots(archives_dir, pattern=pattern):\n",
    "        res = extract_s_id_and_pts(root)\n",
    "        if not res:\n",
    "            continue\n",
    "\n",
    "        station_name, stop_rows = res\n",
    "        if not station_name:\n",
    "            continue\n",
    "\n",
    "        if station_name in match_cache:\n",
    "            eva, is_ambiguous = match_cache[station_name]\n",
    "        else:\n",
    "            eva, score, matched_name, is_ambiguous = best_station_match(\n",
    "                station_name, stations, token_index,\n",
    "                threshold=MATCH_THRESHOLD,\n",
    "                ambiguity_delta=AMBIGUITY_DELTA,\n",
    "            )\n",
    "            match_cache[station_name] = (eva, is_ambiguous)\n",
    "\n",
    "        if eva is None:\n",
    "            unmatched_station_names.add(station_name)\n",
    "            continue\n",
    "        if is_ambiguous:\n",
    "            ambiguous_station_names.add(station_name)\n",
    "            continue\n",
    "\n",
    "        for r in stop_rows:\n",
    "            stop_id = r.get(\"id\")\n",
    "            if not stop_id:\n",
    "                continue\n",
    "\n",
    "            ar_ts = parse_pt(r.get(\"ar_pt\"), tz)\n",
    "            dp_ts = parse_pt(r.get(\"dp_pt\"), tz)\n",
    "\n",
    "            batch[stop_id] = (stop_id, eva, ar_ts, dp_ts)\n",
    "            total_seen_stops += 1\n",
    "\n",
    "            if len(batch) >= BATCH_SIZE:\n",
    "                total_upserted += flush_batch(conn, list(batch.values()), use_execute_values=True)\n",
    "                batch.clear()\n",
    "\n",
    "    total_upserted += flush_batch(conn, list(batch.values()), use_execute_values=True)\n",
    "    batch.clear()\n",
    "\n",
    "    return {\n",
    "        \"total_seen_stops\": total_seen_stops,\n",
    "        \"total_upserted\": total_upserted,\n",
    "        \"unmatched_station_names\": unmatched_station_names,\n",
    "        \"ambiguous_station_names\": ambiguous_station_names,\n",
    "        \"match_cache_size\": len(match_cache),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cb75aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen stops: 2104080\n",
      "Upserted stops: 2094318\n",
      "Unmatched stations: 0\n",
      "Ambiguous stations: 0\n",
      "Beispiele unmatched: []\n",
      "Beispiele ambiguous: []\n"
     ]
    }
   ],
   "source": [
    "result = import_stops_from_archives(conn, archives_dir=TIMETABLES_PATH)\n",
    "print(\"Seen stops:\", result[\"total_seen_stops\"])\n",
    "print(\"Upserted stops:\", result[\"total_upserted\"])\n",
    "print(\"Unmatched stations:\", len(result[\"unmatched_station_names\"]))\n",
    "print(\"Ambiguous stations:\", len(result[\"ambiguous_station_names\"]))\n",
    "\n",
    "# z.B. anzeigen\n",
    "print(\"Beispiele unmatched:\", list(sorted(result[\"unmatched_station_names\"]))[:20])\n",
    "print(\"Beispiele ambiguous:\", list(sorted(result[\"ambiguous_station_names\"]))[:20])\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222575f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
