{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "440535e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Iterator\n",
    "\n",
    "# Iterator Funktion\n",
    "def iter_xml_roots(archives_dir: str | Path, pattern: str = \"*.tar.gz\"\n",
    "                   ) -> Iterator[tuple[Path, str, ET.Element]]:\n",
    "    \"\"\"\n",
    "    Iteriert über alle XML-Dateien in allen .tar.gz-Archiven in archives_dir.\n",
    "\n",
    "    Yields:\n",
    "        (archive_path, xml_member_name, xml_root_element)\n",
    "    \"\"\"\n",
    "    archives_dir = Path(archives_dir)\n",
    "\n",
    "    for archive_path in sorted(archives_dir.glob(pattern)):\n",
    "        try:\n",
    "            with tarfile.open(archive_path, mode=\"r:gz\") as tar:\n",
    "                # Iteriert streamend über Members (speichersparender als getmembers())\n",
    "                for member in tar:\n",
    "                    if not member.isfile():\n",
    "                        continue\n",
    "                    if not member.name.lower().endswith(\".xml\"):\n",
    "                        continue\n",
    "\n",
    "                    extracted = tar.extractfile(member)\n",
    "                    if extracted is None:\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        with extracted as f:\n",
    "                            tree = ET.parse(f)\n",
    "                            yield archive_path, member.name, tree.getroot()\n",
    "                    except ET.ParseError as e:\n",
    "                        print(f\"[WARN] XML ParseError in {archive_path}::{member.name}: {e}\")\n",
    "\n",
    "        except (tarfile.ReadError, OSError) as e:\n",
    "            print(f\"[WARN] Konnte Archiv nicht lesen: {archive_path} ({e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c840ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_s_id_and_pts(root: ET.Element):\n",
    "    \"\"\"\n",
    "    Returns a list of dicts:\n",
    "      { \"id\": <s/@id>, \"ar_pt\": <ar/@pt or None>, \"dp_pt\": <dp/@pt or None> }\n",
    "    \"\"\"\n",
    "    station_name = root.get(\"station\", None)\n",
    "    if not station_name:\n",
    "        return\n",
    "    rows = []\n",
    "    for s in root.findall(\"s\"):\n",
    "        s_id = s.get(\"id\")\n",
    "\n",
    "        ar = s.find(\"ar\")\n",
    "        dp = s.find(\"dp\")\n",
    "\n",
    "        ar_pt = ar.get(\"pt\") if ar is not None else None\n",
    "        dp_pt = dp.get(\"pt\") if dp is not None else None\n",
    "\n",
    "        rows.append({\"id\": s_id, \"ar_pt\": ar_pt, \"dp_pt\": dp_pt})\n",
    "    return station_name, rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e3c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMETABLES_PATH = \"../timetables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964beb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "conn = psycopg2.connect(host=\"localhost\", dbname=\"postgres\", user=\"postgres\", password=\"1234\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a529823",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, data_name, xml_root in iter_xml_roots(TIMETABLES_PATH):\n",
    "    result = extract_s_id_and_pts(xml_root)\n",
    "    if result is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b47faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_stops_table(conn):\n",
    "    create_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS stops (\n",
    "        xml_member_name TEXT NOT NULL,\n",
    "        stop_id         TEXT NOT NULL,\n",
    "\n",
    "        eva             BIGINT NOT NULL REFERENCES stationen(eva),\n",
    "        ar_ts           TIMESTAMPTZ,\n",
    "        dp_ts           TIMESTAMPTZ,\n",
    "\n",
    "        PRIMARY KEY (xml_member_name, stop_id),\n",
    "        CHECK (ar_ts IS NULL OR dp_ts IS NULL OR dp_ts >= ar_ts)\n",
    "    );\n",
    "\n",
    "    CREATE INDEX IF NOT EXISTS stops_eva_ar_idx\n",
    "      ON stops (eva, ar_ts);\n",
    "\n",
    "    CREATE INDEX IF NOT EXISTS stops_stop_id_idx\n",
    "      ON stops (stop_id);\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(create_sql)\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e82c492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import difflib\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "try:\n",
    "    # schneller für große Batches\n",
    "    from psycopg2.extras import execute_values\n",
    "except Exception:\n",
    "    execute_values = None\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Einstellungen\n",
    "# ----------------------------\n",
    "TIMEZONE = \"Europe/Berlin\"\n",
    "\n",
    "MATCH_THRESHOLD = 0.85      # <- ab welcher \"Genauigkeit\" gematcht wird (0..1)\n",
    "AMBIGUITY_DELTA = 0.02      # wenn best - second_best < delta -> \"ambiguous\"\n",
    "BATCH_SIZE = 5000           # Insert-Batches\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Parsing von pt: YYMMDDHHMM -> aware datetime\n",
    "# ----------------------------\n",
    "_pt_re = re.compile(r\"^\\d{10}$\")  # YYMMDDHHMM\n",
    "\n",
    "def parse_pt(pt: str | None, tz: ZoneInfo) -> datetime | None:\n",
    "    if pt is None:\n",
    "        return None\n",
    "    pt = pt.strip()\n",
    "    if not _pt_re.match(pt):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        yy = int(pt[0:2])\n",
    "        year = 2000 + yy\n",
    "        month = int(pt[2:4])\n",
    "        day = int(pt[4:6])\n",
    "        hour = int(pt[6:8])\n",
    "        minute = int(pt[8:10])\n",
    "        return datetime(year, month, day, hour, minute, tzinfo=tz)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Normalisierung & Matching\n",
    "# ----------------------------\n",
    "_umlaut_map = str.maketrans({\"ä\": \"ae\", \"ö\": \"oe\", \"ü\": \"ue\", \"ß\": \"ss\"})\n",
    "_punct_re = re.compile(r\"[^a-z0-9\\s]\")\n",
    "_ws_re = re.compile(r\"\\s+\")\n",
    "\n",
    "# Achtung: name wird in normalize_name() lowercased, daher alles lowercase\n",
    "_stopwords = {\n",
    "    \"berlin\", \"s\", \"u\", \"s+u\", \"u+s\",\n",
    "}\n",
    "\n",
    "def normalize_name(name: str) -> str:\n",
    "    name = name.lower().translate(_umlaut_map)\n",
    "\n",
    "    # --- Synonyme/Abkürzungen vereinheitlichen ---\n",
    "    # Straße (Vollform)\n",
    "    name = name.replace(\"straße\", \"strasse\")\n",
    "\n",
    "    # Betriebsbahnhof\n",
    "    name = name.replace(\"betriebsbf\", \"betriebsbahnhof\")\n",
    "\n",
    "    # Hauptbahnhof -> hbf\n",
    "    name = name.replace(\"hauptbahnhof\", \"hbf\")\n",
    "\n",
    "    # Bahnhof/Bf vereinheitlichen\n",
    "    name = re.sub(r\"\\bbahnhof\\b\", \"bf\", name)\n",
    "    name = re.sub(r\"\\bbhf\\b\", \"bf\", name)\n",
    "    name = re.sub(r\"\\bbf\\.?\\b\", \"bf\", name)\n",
    "\n",
    "    # --- allgemeine Bereinigung ---\n",
    "    name = name.replace(\"&\", \" und \")\n",
    "    name = _punct_re.sub(\" \", name)   # entfernt ., -, etc.\n",
    "\n",
    "    # Jetzt: \"str.\" / \"str\" als Abkürzung expandieren (auch wenn angeklebt)\n",
    "    # Beispiele: \"feuerbachstr.\" -> \"feuerbachstr\" -> \"feuerbachstrasse\"\n",
    "    name = re.sub(r\"(?<=\\w)str\\b\", \"strasse\", name)  # angeklebt am Wortende\n",
    "    name = re.sub(r\"\\bstr\\b\", \"strasse\", name)       # eigenes Token\n",
    "\n",
    "    # Komposita auftrennen: \"...strasse\" -> \"... strasse\"\n",
    "    name = re.sub(r\"(?<!\\s)strasse\\b\", \" strasse\", name)\n",
    "\n",
    "    name = _ws_re.sub(\" \", name).strip()\n",
    "    tokens = [t for t in name.split() if t and t not in _stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "\n",
    "def token_set(s: str) -> set[str]:\n",
    "    return {t for t in s.split() if len(t) >= 2}\n",
    "\n",
    "def similarity(a: str, b: str) -> float:\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    if a == b:\n",
    "        return 1.0\n",
    "\n",
    "    # 1) substring heuristic (gut für \"berlin alexanderplatz\" vs \"alexanderplatz\")\n",
    "    if a in b or b in a:\n",
    "        short, long = (a, b) if len(a) <= len(b) else (b, a)\n",
    "        substring_score = 0.90 + 0.10 * (len(short) / max(1, len(long)))\n",
    "    else:\n",
    "        substring_score = 0.0\n",
    "\n",
    "    # 2) sequence similarity\n",
    "    seq_score = difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "    # 3) token overlap (F1-ähnlich)\n",
    "    ta, tb = token_set(a), token_set(b)\n",
    "    token_score = (2 * len(ta & tb) / (len(ta) + len(tb))) if ta and tb else 0.0\n",
    "\n",
    "    return max(substring_score, seq_score, token_score)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class StationRec:\n",
    "    eva: int\n",
    "    raw_name: str\n",
    "    norm_name: str\n",
    "    tokens: tuple[str, ...]\n",
    "\n",
    "\n",
    "def load_stations(conn) -> list[StationRec]:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT eva, name FROM stationen;\")\n",
    "        rows = cur.fetchall()\n",
    "\n",
    "    stations: list[StationRec] = []\n",
    "    for eva, name in rows:\n",
    "        if not name:\n",
    "            continue\n",
    "        norm = normalize_name(name)\n",
    "        toks = tuple(sorted(token_set(norm)))\n",
    "        stations.append(StationRec(int(eva), name, norm, toks))\n",
    "    return stations\n",
    "\n",
    "\n",
    "def build_token_index(stations: list[StationRec]) -> dict[str, list[int]]:\n",
    "    \"\"\"token -> list of indices into `stations`\"\"\"\n",
    "    idx: dict[str, list[int]] = {}\n",
    "    for i, st in enumerate(stations):\n",
    "        for t in st.tokens:\n",
    "            idx.setdefault(t, []).append(i)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def best_station_match(\n",
    "    query_name: str,\n",
    "    stations: list[StationRec],\n",
    "    token_index: dict[str, list[int]],\n",
    "    threshold: float,\n",
    "    ambiguity_delta: float,\n",
    ") -> tuple[int | None, float, str | None, bool]:\n",
    "    \"\"\"\n",
    "    Returns (eva_or_None, best_score, matched_raw_name, is_ambiguous)\n",
    "    \"\"\"\n",
    "    q_norm = normalize_name(query_name)\n",
    "    if not q_norm:\n",
    "        return None, 0.0, None, False\n",
    "\n",
    "    q_tokens = token_set(q_norm)\n",
    "\n",
    "    # Kandidaten über Token-Index einschränken\n",
    "    cand_indices: set[int] = set()\n",
    "    for t in q_tokens:\n",
    "        cand_indices.update(token_index.get(t, []))\n",
    "\n",
    "    # Fallback: wenn gar keine Tokens/keine Kandidaten -> global scan\n",
    "    if not cand_indices:\n",
    "        cand_indices = set(range(len(stations)))\n",
    "\n",
    "    best_eva: int | None = None\n",
    "    best_score: float = 0.0\n",
    "    best_name: str | None = None\n",
    "    best_norm: str | None = None\n",
    "\n",
    "    second_best_score: float = 0.0\n",
    "\n",
    "    for i in cand_indices:\n",
    "        st = stations[i]\n",
    "        sc = similarity(q_norm, st.norm_name)\n",
    "\n",
    "        if sc > best_score:\n",
    "            second_best_score = best_score\n",
    "            best_eva, best_score, best_name, best_norm = st.eva, sc, st.raw_name, st.norm_name\n",
    "        elif sc == best_score and best_norm is not None:\n",
    "            # Tie-break: kürzere norm_name bevorzugen (hilft z.B. \"hbf\" vs \"hbf tief\")\n",
    "            if len(st.norm_name) < len(best_norm):\n",
    "                best_eva, best_score, best_name, best_norm = st.eva, sc, st.raw_name, st.norm_name\n",
    "        elif sc > second_best_score:\n",
    "            second_best_score = sc\n",
    "\n",
    "    if best_eva is None or best_score < threshold:\n",
    "        return None, best_score, best_name, False\n",
    "\n",
    "    is_ambiguous = (best_score - second_best_score) < ambiguity_delta\n",
    "    return best_eva, best_score, best_name, is_ambiguous\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# DB Insert/Upsert Stops\n",
    "# ----------------------------\n",
    "UPSERT_SQL = \"\"\"\n",
    "INSERT INTO stops (xml_member_name, stop_id, eva, ar_ts, dp_ts)\n",
    "VALUES %s\n",
    "ON CONFLICT (xml_member_name, stop_id) DO UPDATE\n",
    "SET eva   = EXCLUDED.eva,\n",
    "    ar_ts = EXCLUDED.ar_ts,\n",
    "    dp_ts = EXCLUDED.dp_ts;\n",
    "\"\"\"\n",
    "\n",
    "def flush_batch(conn, batch: list[tuple], use_execute_values: bool = True) -> int:\n",
    "    if not batch:\n",
    "        return 0\n",
    "    with conn.cursor() as cur:\n",
    "        if use_execute_values and execute_values is not None:\n",
    "            execute_values(cur, UPSERT_SQL, batch, page_size=2000)\n",
    "        else:\n",
    "            # Fallback ohne execute_values (muss auch das Composite-Key-Schema verwenden!)\n",
    "            upsert_one = \"\"\"\n",
    "            INSERT INTO stops (xml_member_name, stop_id, eva, ar_ts, dp_ts)\n",
    "            VALUES (%s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (xml_member_name, stop_id) DO UPDATE\n",
    "            SET eva = EXCLUDED.eva, ar_ts = EXCLUDED.ar_ts, dp_ts = EXCLUDED.dp_ts;\n",
    "            \"\"\"\n",
    "            cur.executemany(upsert_one, batch)\n",
    "    conn.commit()\n",
    "    return len(batch)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main Import\n",
    "# ----------------------------\n",
    "def import_stops_from_archives(conn, archives_dir: str, pattern: str = \"*.tar.gz\"):\n",
    "    ensure_stops_table(conn)\n",
    "\n",
    "    tz = ZoneInfo(TIMEZONE)\n",
    "\n",
    "    stations = load_stations(conn)\n",
    "    token_index = build_token_index(stations)\n",
    "\n",
    "    unmatched_station_names: set[str] = set()\n",
    "    ambiguous_station_names: set[str] = set()\n",
    "\n",
    "    # Cache, damit gleiche Station nicht immer neu gematcht wird\n",
    "    match_cache: dict[str, tuple[int | None, bool]] = {}  # name -> (eva_or_None, ambiguous)\n",
    "\n",
    "    # Optional: dedupe innerhalb eines Batches auf Composite-Key (verhindert CardinalityViolation)\n",
    "    batch: dict[tuple[str, str], tuple] = {}  # (xml_member_name, stop_id) -> row\n",
    "\n",
    "    total_upserted = 0\n",
    "    total_seen_stops = 0\n",
    "\n",
    "    for archive_path, xml_member_name, root in iter_xml_roots(archives_dir, pattern=pattern):\n",
    "        res = extract_s_id_and_pts(root)\n",
    "        if not res:\n",
    "            continue\n",
    "\n",
    "        station_name, stop_rows = res\n",
    "        if not station_name:\n",
    "            continue\n",
    "\n",
    "        if station_name in match_cache:\n",
    "            eva, is_ambiguous = match_cache[station_name]\n",
    "        else:\n",
    "            eva, score, matched_name, is_ambiguous = best_station_match(\n",
    "                station_name, stations, token_index,\n",
    "                threshold=MATCH_THRESHOLD,\n",
    "                ambiguity_delta=AMBIGUITY_DELTA,\n",
    "            )\n",
    "            match_cache[station_name] = (eva, is_ambiguous)\n",
    "\n",
    "        if eva is None:\n",
    "            unmatched_station_names.add(station_name)\n",
    "            continue\n",
    "        if is_ambiguous:\n",
    "            ambiguous_station_names.add(station_name)\n",
    "            continue\n",
    "\n",
    "        ap = str(archive_path)\n",
    "\n",
    "        for r in stop_rows:\n",
    "            stop_id = r.get(\"id\")\n",
    "            if not stop_id:\n",
    "                continue\n",
    "\n",
    "            ar_ts = parse_pt(r.get(\"ar_pt\"), tz)\n",
    "            dp_ts = parse_pt(r.get(\"dp_pt\"), tz)\n",
    "\n",
    "            key = (xml_member_name, stop_id)\n",
    "            batch[key] = (xml_member_name, stop_id, eva, ar_ts, dp_ts)\n",
    "            total_seen_stops += 1\n",
    "\n",
    "            if len(batch) >= BATCH_SIZE:\n",
    "                total_upserted += flush_batch(conn, list(batch.values()), use_execute_values=True)\n",
    "                batch.clear()\n",
    "\n",
    "    # Rest flushen\n",
    "    total_upserted += flush_batch(conn, list(batch.values()), use_execute_values=True)\n",
    "    batch.clear()\n",
    "\n",
    "    return {\n",
    "        \"total_seen_stops\": total_seen_stops,\n",
    "        \"total_upserted\": total_upserted,\n",
    "        \"unmatched_station_names\": unmatched_station_names,\n",
    "        \"ambiguous_station_names\": ambiguous_station_names,\n",
    "        \"match_cache_size\": len(match_cache),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cb75aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen stops: 2104080\n",
      "Upserted stops: 2104080\n",
      "Unmatched stations: 0\n",
      "Ambiguous stations: 0\n",
      "Beispiele unmatched: []\n",
      "Beispiele ambiguous: []\n"
     ]
    }
   ],
   "source": [
    "result = import_stops_from_archives(conn, archives_dir=TIMETABLES_PATH)\n",
    "print(\"Seen stops:\", result[\"total_seen_stops\"])\n",
    "print(\"Upserted stops:\", result[\"total_upserted\"])\n",
    "print(\"Unmatched stations:\", len(result[\"unmatched_station_names\"]))\n",
    "print(\"Ambiguous stations:\", len(result[\"ambiguous_station_names\"]))\n",
    "\n",
    "# z.B. anzeigen\n",
    "print(\"Beispiele unmatched:\", list(sorted(result[\"unmatched_station_names\"]))[:20])\n",
    "print(\"Beispiele ambiguous:\", list(sorted(result[\"ambiguous_station_names\"]))[:20])\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222575f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
